1. 拷包 /home/xxproject/lib/hadoop-2.7.4.tar.gz
2. 解压，建软连，ln -sf ~/lib/hadoop-2.7.4 ~/lib/hadoop
3. 机器规划
HOSTNAME	IP	    NameNode	        DataNode	    YARN
node-01	10.20.0.11	NameNode		    ResourceManager
node-02	10.20.0.12	SecondaryNameNode	DataNode	    NodeManager
node-03	10.20.0.13		                DataNode	    NodeManager
node-04	10.20.0.14		                DataNode	    NodeManager

4. 修改配置文件
5. etc/hadoop/core-site.xml
<configuration>
    <!-- 指定namenode的主机名和端口号，此处的端口号需要和hdfs-site.xml的dfs.namenode.rpc-address配置的端口相同 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node-01:9000</value>
        <description>The name of the default file system.</description>
    </property>
    <!-- 配置hadoop的临时目录，我们不需要把这个目录创建出来, 这里的路径默认是NameNode,DataNode,JournalNode等存放数据的公共目录-->  
    <property>  
        <name>hadoop.tmp.dir</name>  
        <value>/home/xxproject/data/hadoop/tmp</value>  
        <description>A base for other temporary directories.</description>    
    </property>
</configuration>
6. etc/hadoop/hdfs-site.xml
<configuration>
    <!-- 这里是secondary namenode的主机名、端口，可以通过HTTP/HTTPS访问 -->
    <property>  
        <name>dfs.namenode.secondary.http-address</name>  
        <value>node-02:50090</value>  
        <description>The secondary namenode http server address and port. </description>  
    </property>
    <property>  
        <name>dfs.namenode.secondary.https-address</name>
        <value>node-02:50091</value>
        <description>The secondary namenode HTTPS server address and port. </description> 
    </property> 
    <!-- 配置web UI 访问hdfs系统的地址-->  
    <property>  
        <name>dfs.namenode.http-address</name>  
        <value>node-01:50070</value>  
        <description>The address and the base port where the dfs namenode web ui will listen on. </description>  
    </property>   
    <!-- 设置HDFS的副本数 -->  
    <property>  
        <name>dfs.replication</name>  
        <value>3</value>  
    </property>
</configuration>
7. etc/hadoop/slaves
node-02
node-03
node-04
8. etc/hadoop/masters
node-02
9. /etc/hosts
10.20.0.11  node-01
10.20.0.12  node-02
10.20.0.13  node-03
10.20.0.14  node-04
10. 环境变量修改
echo '
# !!!No Modification, This Section is Auto Generated by Hadoop
export HADOOP_HOME=/home/xxproject/lib/hadoop
export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
' >> ~/.bash_profile
source ~/.bash_profile
11.其他所有节点拷贝同样的配置，需要配置lib/hadoop/etc/hadoop/hadoop-env.sh中的JAVA_HOME
12. hdfs namenode -format -force/hadoop namenode -format -force/http://blog.csdn.net/gis_101/article/details/52679914/不要随意多次初始化
12. start-dfs.sh
13. NN: http://10.20.0.11:50070/dfshealth.html#tab-overview
14. SNN: http://10.20.0.12:50090/status.html
15. 注意机器的内核参数调优、防火墙关闭、NTP等基础问题
16. mapred-site.xml
<configuration>
    <!-- 指定MR资源调度方式 -->  
    <property>  
        <name>mapreduce.framework.name</name>  
        <value>yarn</value>  
        <description>The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.</description>  
    </property>
    
    <!-- MR默认需要的资源数 -->  
    <property>  
        <name>yarn.app.mapreduce.am.resource.mb</name>  
        <value>1536</value>  
        <description>The amount of memory the MR AppMaster needs.</description>  
    </property>
    <property>  
        <name>yarn.app.mapreduce.am.resource.cpu-vcores</name>  
        <value>1</value>  
        <description>The number of virtual CPU cores the MR AppMaster needs.</description>  
    </property>
</configuration>
17. yarn-site.xml
<configuration>
    <!-- Site specific YARN configuration properties -->
    <!--Resourcemanager的配置,自定ResourceManager的地址，还是单点，这是隐患-->  
    <property>  
        <name>yarn.resourcemanager.hostname</name>  
        <value>node-01</value>  
    </property>  
  
    <!--配置resourcemanager的http地址 The http address of the RM web application.-->  
    <property>  
        <name>yarn.resourcemanager.webapp.address</name>  
        <value>node-01:8088</value>  
    </property>  
  
    <!--配置resourcemanager的https地址 The https adddress of the RM web application.-->  
    <property>  
        <name>yarn.resourcemanager.webapp.https.address</name>  
        <value>node-01:8090</value>  
    </property>  
  
    <!-- 在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的 -->  
    <property>  
        <name>yarn.nodemanager.aux-services</name>  
        <value>mapreduce_shuffle</value>  
    </property>
    
    <!--新版本需要配置可用的资源数, Amount of physical memory, in MB, that can be allocated for containers.-->  
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>1</value>
    </property>
</configuration>
18. start-yarn.sh
19. stop-all.sh/start-all.sh/jps

测试一下--以下HDFS的前缀可以不要--本来就是在NN上执行的
hadoop fs -ls hdfs://node-01:9000/
hadoop fs -put lib/hadoop-2.7.4.tar.gz hdfs://node-01:9000/
hdfs dfs -get hdfs://node-01:9000/hadoop-2.7.4.tar.gz .
hadoop jar lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar pi 5 10

# word-count测试
touch word-count.txt    \
    && echo "hello world" >> word-count.txt \
    && echo "hello tom" >> word-count.txt \
    && echo "hello jim" >> word-count.txt \
    && echo "hello kitty" >> word-count.txt \
    && echo "hello baby" >> word-count.txt
hadoop fs -mkdir hdfs://node-01:9000/word-count
hadoop fs -mkdir hdfs://node-01:9000/word-count/input
hadoop fs -put word-count.txt hdfs://node-01:9000/word-count/input
hadoop jar lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar wordcount hdfs://node-01:9000/word-count/input hdfs://node-01:9000/word-count/output
hadoop fs -ls hdfs://node-01:9000/word-count/output
hadoop fs -cat hdfs://node-01:9000/word-count/output/part-r-00000

